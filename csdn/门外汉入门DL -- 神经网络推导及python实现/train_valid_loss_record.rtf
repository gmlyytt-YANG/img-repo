{\rtf1\ansi\ansicpg936\cocoartf2509
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\paperw11900\paperh16840\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\fs24 \cf0 no regularization:\
\
iteration 1550, train loss: 0.3438481091130844, valid_loss: 2.5721885881022963\
iteration 1600, train loss: 0.3432821160251056, valid_loss: 2.5690724565021563\
iteration 1650, train loss: 0.3427336549909239, valid_loss: 2.5661311700572114\
iteration 1700, train loss: 0.342199564200028, valid_loss: 2.5633477608313555\
iteration 1750, train loss: 0.34167690238274945, valid_loss: 2.560706537052332\
iteration 1800, train loss: 0.3411629322898378, valid_loss: 2.5581929841677735\
iteration 1850, train loss: 0.3406551052667187, valid_loss: 2.555793673221364\
iteration 1900, train loss: 0.3401510468860704, valid_loss: 2.553496176063224\
iteration 1950, train loss: 0.3396485436233242, valid_loss: 2.551288986977024\
iteration 1999, train loss: 0.3391556084995299, valid_loss: 2.5492032855573794\
\
L2 regularization:\
iteration 1300, train loss: 1.09977712566086, valid_loss: 0.21506450810326314\
iteration 1350, train loss: 1.09977712566086, valid_loss: 0.21506450810326314\
iteration 1400, train loss: 1.09977712566086, valid_loss: 0.21506450810326314\
iteration 1450, train loss: 1.09977712566086, valid_loss: 0.21506450810326314\
iteration 1500, train loss: 1.09977712566086, valid_loss: 0.21506450810326314\
iteration 1550, train loss: 1.09977712566086, valid_loss: 0.21506450810326314\
iteration 1600, train loss: 1.09977712566086, valid_loss: 0.21506450810326314\
\
\
L1 regularization:\
iteration 1600, train loss: 0.20726140239368068, valid_loss: 0.019843994548783522\
iteration 1650, train loss: 0.20426955913962874, valid_loss: 0.019351285605672435\
iteration 1700, train loss: 0.19911233804205428, valid_loss: 0.018032122910000836\
iteration 1750, train loss: 0.17876171285175943, valid_loss: 0.01476366499278606\
iteration 1800, train loss: 0.1528429869705662, valid_loss: 0.010115850156599434\
iteration 1850, train loss: 0.13332892760140821, valid_loss: 0.0068871782947589515\
iteration 1900, train loss: 0.11506716440296294, valid_loss: 0.004298045667616563\
iteration 1950, train loss: 0.09654267332402698, valid_loss: 0.0022991280118166628\
\
}