{\rtf1\ansi\ansicpg936\cocoartf2509
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\fnil\fcharset134 PingFangSC-Regular;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\paperw11900\paperh16840\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\fs24 \cf0 ## softmax_loss backward\
\

\f1 \'c9\'cf\'ce\'c4\'b5\'c4
\f0 svm_loss 
\f1 \'d2\'d1\'be\'ad\'b0\'d1\'d1\'f9\'b1\'be\'b5\'c4\'d7\'bc\'b1\'b8\'c7\'e9\'bf\'f6\'bd\'bb\'b4\'fa\'c7\'e5\'b3\'fe\'c1\'cb\'a3\'ac\'b6\'f8\'ba\'f3\'d5\'e2\'d2\'bb\'bd\'da\'c0\'b4\'bd\'e9\'c9\'dc
\f0 softmax_loss, 
\f1 \'be\'df\'cc\'e5\'b5\'c4\'cd\'c6\'b5\'bc\'cf\'b8\'bd\'da\'be\'cd\'b2\'bb\'d4\'d9\'d7\'b8\'ca\'f6\'c1\'cb\'a3\'ac\'a3\'a8\'c8\'e7\'b9\'fb\'b4\'f3\'bc\'d2\'cf\'eb\'c1\'cb\'bd\'e2\'b5\'c4\'d4\'d9\'c7\'e5\'b3\'fe\'d2\'bb\'b5\'e3\'a3\'ac\'c7\'eb\'b2\'ce\'d5\'d5
\f0 cs231n lecture3
\f1 \'b5\'c4\'b9\'d8\'d3\'da
\f0 softmax_loss
\f1 \'b5\'c4\'bd\'e2\'ca\'cd\'a3\'ac\'d2\'b2\'bf\'c9\'d2\'d4\'b2\'ce\'d5\'d5
\f0 cs229
\f1 \'b5\'c4
\f0 softmax
\f1 \'cf\'ea\'bd\'e2\'a3\'a9\'a3\'ac
\f0 \
\

\f1 \'ba\'cd\'c9\'cf\'ce\'c4\'c0\'e0\'cb\'c6\'a3\'ac\'be\'ad\'b9\'fd\'d2\'bb\'cf\'b5\'c1\'d0\'cd\'c6\'b5\'bc\'b5\'c3\'b5\'bd\'b5\'c4\'b5\'c3\'b7\'d6\'cf\'f2\'c1\'bf\'b9\'ab\'ca\'bd\'c8\'e7\'cf\'c2\'a3\'ba
\f0 \
$$P(Y=k|X=x_\{i\})=\\frac\{e^\{s_\{k\}\}\}\{\\sum_\{j\}e^\{s_\{j\}\}\}$$\

\f1 \'b6\'f8
\f0 $L_\{i\}$
\f1 \'b1\'ed\'ca\'be\'c8\'e7\'cf\'c2\'a3\'ba
\f0 \
$$L_\{i\}=-logP(Y=y_\{i\}|X=x_\{i\})$$\
\

\f1 \'b6\'f8\'cb\'f9\'d3\'d0\'d1\'f9\'b1\'be\'b5\'c4
\f0 softmax_loss
\f1 \'ca\'c7\'c8\'e7\'cf\'c2\'b9\'ab\'ca\'bd\'a3\'ba
\f0 \
$$L=\\frac\{1\}\{N\}\\sum_\{i=1\}^\{N\}L_\{i\}$$\
\

\f1 \'d0\'e8\'d2\'aa\'c7\'f3\'bd\'e2
\f0 $ds_\{j\}(j\\neq y_\{i\})$
\f1 \'ba\'cd
\f0 $ds_\{y_\{i\}\}$,
\f1 \'b6\'d4\'d3\'da
\f0 $ds_\{j\}(j\\neq y_\{i\})$
\f1 \'a3\'ac\'b8\'f9\'be\'dd\'c7\'f3\'b5\'bc\'b7\'a8\'d4\'f2\'d3\'d0\'c8\'e7\'cf\'c2\'b9\'ab\'ca\'bd\'a3\'ba
\f0 \
$$\\frac\{\\partial L_\{i\}\}\{\\partial s_\{j\}\}=-\\frac\{\\sum_\{j\}e^\{s_\{j\}\}\}\{e^\{s_\{y_\{i\}\}\}\}\\cdot (-\\frac\{e^\{s_\{y_i\}\}\\cdot e^\{s_\{j\}\}\}\{(\\sum_\{j\}e^\{s_\{j\}\})^\{2\}\})=\\frac\{e^\{s_\{j\}\}\}\{\\sum_\{j\}e^\{s_\{j\}\}\}$$\
\

\f1 \'b6\'f8\'b6\'d4\'d3\'da
\f0 $ds_\{y_\{i\}\}$
\f1 \'a3\'ac\'c7\'f3\'bd\'e2\'c6\'f0\'c0\'b4\'cf\'e0\'b6\'d4\'c0\'a7\'c4\'d1\'a3\'ac\'ca\'a1\'c2\'d4\'d6\'d0\'bc\'e4\'b2\'bd\'d6\'e8\'a3\'ac\'d3\'d0\'c8\'e7\'cf\'c2\'b9\'ab\'ca\'bd\'a3\'ba
\f0 \
$$\\frac\{\\partial L_\{i\}\}\{\\partial s_\{y_\{i\}\}\}=\\frac\{e^\{s_\{y_\{i\}\}\}\}\{\\sum_\{j\}e^\{s_\{j\}\}\}-1$$\
\

\f1 \'cf\'a3\'cd\'fb\'b4\'f3\'bc\'d2\'c4\'dc\'b9\'bb\'d7\'d0\'cf\'b8\'cd\'c6\'b5\'bc\'a3\'ac
\f0  
\f1 \'cf\'c2\'c3\'e6\'ca\'c7
\f0 python
\f1 \'b5\'c4\'b4\'fa\'c2\'eb\'b1\'ed\'ca\'be\'a3\'ba
\f0 \
```Python\
def softmax_loss(x, y):    \
    """    \
    Computes the loss and gradient for softmax classification.    Inputs:    \
    - x: Input data, of shape (N, C) where x[i, j] is the score for the jth class         \
    for the ith input.    \
    - y: Vector of labels, of shape (N,) where y[i] is the label for x[i] and         \
         0 <= y[i] < C   \
    Returns a tuple of:    \
    - loss: Scalar giving the loss    \
    - dx: Gradient of the loss with respect to x   \
    """    \
    probs = np.exp(x - np.max(x, axis=1, keepdims=True))    \
    probs /= np.sum(probs, axis=1, keepdims=True)    \
    N = x.shape[0]   \
    loss = -np.sum(np.log(probs[np.arange(N), y])) / N    \
    dx = probs.copy()    \
    dx[np.arange(N), y] -= 1    \
    dx /= N    \
\
    return loss, dx\
```}